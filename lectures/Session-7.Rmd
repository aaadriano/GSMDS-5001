---
title: "A/B Testing"
author: "Jameson Watts, Ph.D."
output:
  ioslides_presentation:
    smaller: yes
    widescreen: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE)
```

## Agenda

1. Regression Review
2. A/B Testing
3. 1 on 1 consultations (hopefully)

## Environment setup
```{r}
library(tidyverse)
library(moderndive)
data(evals)
```

# Regression Review

## Practice
1. Load the "evals" dataset from the moderndive library with the command $data(evals)$
2. Pair up and create a linear model that uses **bty_avg** to predict **score** 
3. Transform the dependent and/or independent variables as necessary
4. Create a sentence that explains the magnitude of the relationship
5. Graph the residuals for adherence to assumptions of linear regression

- Linearity
- Independence
- Normality
- Equality

## Data overview

```{r}
glimpse(evals)
```

## Histograms
<div class="columns-2">
```{r, fig.width=4}
ggplot(evals, aes(score))+geom_histogram()
ggplot(evals, aes(bty_avg))+geom_histogram()
```

</div>

## Correlations

```{r}
summarise(evals,correlation=cor(score,bty_avg))$correlation
summarise(evals,correlation=cor(log(score),bty_avg))$correlation
summarise(evals,correlation=cor(score,log(bty_avg)))$correlation
summarise(evals,correlation=cor(log(score),log(bty_avg)))$correlation
```

## Linearity
```{r}
evals %>% 
  ggplot(aes(log(bty_avg), log(score)))+
  geom_point(alpha=.5)
```


## Models
```{r}
estimates = list()
estimates[[1]] <- get_regression_table(lm(score~bty_avg, data = evals))$estimate[2]
estimates[[2]] <- get_regression_table(lm(log(score+1)~bty_avg, data = evals))$estimate[2]
estimates[[3]] <- get_regression_table(lm(score~log(bty_avg+1), data = evals))$estimate[2]
estimates[[4]] <- get_regression_table(lm(log(score+1)~log(bty_avg+1), data = evals))$estimate[2]
for(e in 1:4){
  print(estimates[[e]])
}
```

- plain ~ plain: 1 point increase in EV -> $x$ change in DV
- log ~ plain: 1 point increase in EV -> $e^x-1*100$ percent change in DV
- plain ~ log: 1 percent increase in EV -> $x/100$ change in DV
- log ~ log: 1 percent increase in EV -> $x$ percent change in DV

## Normality
```{r}
get_regression_points(lm(score~bty_avg, data = evals)) %>% 
  ggplot(aes(residual))+
    geom_histogram()
```


## Residuals

```{r, fig.height=4}
get_regression_points(lm(score~bty_avg, data = evals)) %>% 
  ggplot(aes(bty_avg, residual))+
    geom_point()
```

What about independence?

# Field experiments (A/B testing)

## 
*Credit:* Warm thanks to Elea Feit for content on A/B testing.

![](images/ab_test.png){width=85%}

Source: splitmetrics.com


## Example A/B test

<div class="columns-2">
 ![](images/smartwool.png){width=95%}
 
  1. Randomly assign customers to treatments
  2. Measure response(s) 
  3. Compare groups to determine how the treatment changes response
</div>

 Source: Optimizely Blog


## Why A/B tests work
By **randomizing** over a large number of customers, we create groups that are the same, on average. 

<div class="centered">
![](images/apples.jpg){width=15%}  ![](images/apples.jpg){width=15%}
</div>


Any behavioral differences between these groups is **caused by the treatments** we randomly assigned. 

<div class="centered">
<div class="red2">
1, 2, 3. Repeat with me. Randomization will set you free. 
</div>
</div>

## Example email A/B test
The email A/B test we will analyze was conducted by an online-only wine store. 

![](images/wine_store.png){width=80%}


## Wine retailer email test
**Test setting**: email to retailer customers

**Unit**: customer (email address)

**Treatments**: email version A, email version B, holdout

**Reponse**: open, click and 1-month purchase ($)

**Selection**: all active customers

**Assignment**: randomly assigned (1/3 each)


## Wine retailer email test data
```{r}
setwd("~/GDrive/teaching/GSMDS-5001/")
d <- read.csv("resources/ab-data.csv")
head(d)
```

## Types of variables associated with a test {.build}
- **Treatment indicator** (x's)
    - Which (randomized) treatment was received  
    
- **Response** (y's)
    - Outcome(s) measured for each customer. Aka the DV or dependant variable.  
    
- **Baseline variables** ("z's")
    - Other stuff we know about customers **prior** to the randomization  

<div class="centered">
<div class="red2">
Everything measured after the randomization is an outcome
</div>
</div>


## Treatment indicator
```{r}
d %>% 
  group_by(group) %>% 
  count()
```


## Responses
- open test email (load images)
- click test email to visit website
- purchases ($) in 30 days after email sent
```{r}
summary(d[,c("open", "click", "purch")])
```


## Baseline variables
- days since last purchase
- website visits
- total past purchases ($)
```{r}
summary(d[,c("last_purch", "visits", "past_purch")]) 
```


## More baseline variables
- total past purchases by category ($)
```{r}
summary(d[, c("chard", "sav_blanc", "syrah", "cab")]) 
```

\

<div class="centered">
<div class="red2">
Whoa! That's a lot of chardonnay for one customer!
</div>
</div>

# Analysis of A/B tests

## {.build .bigger .flexbox .vcenter}

<div class="blue">
What is the first question you should ask about an A/B test? 
</div>

<div class="centered">
~~Did the treatment affect the response?~~
</div>

<div class="centered">
Was the randomization done correctly? 
</div>  

\ 

<div class="centered">
<div class="blue">
How could we check the randomization with the data?
</div>
</div>

## Randomization checks 1
Randomization checks confirm that the baseline variables are distributed similarly for the treatment and control groups. 

**Averages of baseline variables by treatment group**

```{r, echo=FALSE}
d %>% 
  group_by(group) %>% 
  summarize(mean(last_purch), mean(visits), mean(past_purch), mean(past_purch > 0))
```

## Randomization checks 2
The distributions of baseline variables should also be the same between groups.

```{r, echo=FALSE, warning=FALSE}
d %>% filter(past_purch > 0) %>% 
ggplot(aes(x=past_purch, fill=group)) + 
  geom_histogram(binwidth = 25, alpha=0.2, position="identity") +
  xlim(0, 2000) + 
  xlab("Past Purchases ($)") + ylab("Customers") + 
  labs(title="Distribution of past purchases by group")
```


## Randomization checks 3

```{r, echo=FALSE, warning=FALSE}
d %>% filter(visits > 0) %>% 
ggplot(aes(x=visits, fill=group)) + 
  geom_histogram(binwidth = 1, alpha=0.2, position="identity") +
  xlim(0, 50) + 
  xlab("Days Since Last Purchase") + ylab("Customers") + 
  labs(title="Distribution of website visits by group")
```


## Exercise

1. Choose a wine category
2. Compare the past purchases in this category to confirm that the randomization produced groups with similar distributions.

## Solution
```{r, echo=FALSE, warning=FALSE}
d %>% filter(chard>0) %>% 
ggplot(aes(x=chard, fill=group)) + 
  geom_histogram(binwidth = 20, alpha=0.2, position="identity") +
  xlim(0, 2000) + 
  xlab("Chardonnay") + ylab("Customers") + 
  labs(title="Distribution of website visits by group")
```


## Did the treatments affect the response?
```{r, echo=FALSE}
d %>% 
  group_by(group) %>% 
  summarize(mean(open), mean(click), mean(purch))
```
Email A looks better for opens and clicks, but maybe not purchases. Both emails seem to generate higher average purchases than the control.


## Email A have higher open rate than email B? 
```{r}
success <- d %>% 
  filter(group!="ctrl") %>% 
  group_by(group,open) %>% 
  count() %>% 
  filter(open==1)
trials <- d %>% 
  filter(group!="ctrl") %>% 
  count(group)

prop.test(success$n,trials$n)
```


## Email A have a higher open rate than email B?
```{r, echo=FALSE}
mosaicplot(xtabs(~ group + open, data=d), 
           main="Wine Retailer Test: Email Opens")
```


## Email A have a higher click rate than email B? 
```{r}
success <- d %>% 
  filter(group!="ctrl") %>% 
  group_by(group,click) %>% 
  count() %>% 
  filter(click==1)
trials <- d %>% 
  filter(group!="ctrl") %>% 
  count(group)

prop.test(success$n,trials$n)
```

Note that we analyze click rate among all who *received* the email. There may be systematic differences in the types of customers who opened email A versus email B. 
 
 
## Email A have higher opens and clicks than email B?
```{r, echo=FALSE, fig.height=4}
d %>% filter(group != "ctrl") %>% group_by(group) %>% 
  summarize(open=mean(open), click=mean(click)) %>%
  gather(response, mean, -group) %>%
  ggplot(aes(fill=response, y=mean, x=group)) + 
  geom_bar(position="dodge", stat="identity") + 
  ylab("Response Rate") + xlab("")
```

- Email A has a significantly higher open rate than email B. The diference in open rate is between 5.7% and 7.0% (95% CI).
- Email A has a significantly higher click rate than email B. The difference is between 3.8% and 4.6% (95% CI).   


## Do any groups have higher average purchases? 
```{r, echo=FALSE, warning=FALSE}
d %>% 
  ggplot(aes(y=purch, x=group)) + 
  geom_boxplot() +
  ylab("30-Day Purchases ($)") + xlab("") + 
  scale_y_log10()
```


## Does groups have higher average purchases? 
```{r, echo=FALSE, warning=FALSE}
d %>%
  ggplot(aes(y=purch, x=group)) + 
  geom_violin() +
  ylab("30-Day Purchases ($)") + xlab("") + 
  scale_y_log10()
```


## Do groups have higher average purchases? 
```{r, echo=FALSE, warning=FALSE}
d %>%
  ggplot(aes(y=purch, x=group)) + 
  geom_dotplot(binaxis='y', stackdir='center',
               stackratio=0.1, dotsize=0.1) +
  ylab("30-Day Purchases ($)") + xlab("")+
  scale_y_log10()
```

## Email A generate more purchases than B? 
```{r}
t.test(purch ~ group, data=d[d$group != "ctrl",])
```
There is not a significant difference in average purchases between email A and email B. 


## Do emails generate higher purchases? 
```{r}
d$email <- d$group == "email_A" | d$group == "email_B"
t.test(purch ~ email, data=d)
```
Those who received an email have \$0.91 to \$1.94 higher average purchases (95% CI).

## Summary of findings (suitable for texting)
- Email A has significantly higher opens and clicks than email B, but purchase are similar for both emails -> Send email A!
- Both emails generate higher average purchases than the control -> Send emails! 

# Design of A/B tests

## Seven key questions 
1. Business question
2. Test setting (lab v. field)
3. Unit of analysis (visit, customer, store)
4. Treatments
5. Response variable(s)
6. Selection of units
7. Assignment to treatments
8. Sample size

<div class="centered">
<div class="blue">
If you can answer these questions, you have a test plan. 
</div>
</div>

## Email test

**Business question**: Does email work? If so which email is better? 

**Test setting**: email to retailer customers

**Unit**: email address

**Treatments**: email version A, email version B, holdout

**Reponse**: open, click and 30-day purchase ($)

**Selection**: all active customers

**Assignment**: randomly assigned (1/3 each)

**Sample size**: 123,988 emails


## Typical website test

**Business question**: Which version of a page? 

**Test setting**: website (field)

**Unit of analysis**: visitor (cookie-tracked)

**Treatments**: versions A and B

**Response variable**: clicks, conversions

**Selection of units**: all who visit

**Assignment to treatments**: random (by testing sw)

**Sample size**: ???

## Sample size planning

Significance tests will erroneously detect effects that aren't there, if you repeatedly test for significance as the data comes in and stop when you get a significant difference. 

```{r}
sig <- rep(0, 1000)
for (r in 1:1000) {
  A <- rnorm(101); B <- rnorm(101)
  pval <- rep(NA, 100)
  for (n in 1:100) pval[n] <- t.test(A[1:(n+1)], B[1:(n+1)])$p.value  # repeated testing
  if (min(pval) < 0.05) sig[r] <- 1  # any significance along the way
}
mean(sig)   # bigger than the nominal significance of 5%
```

 


## Sample size planning {.bigger}

The traditional recommendation is to set the sample size **in advance** and not test for significance until the data comes in.

<div class="centered">
$n_1 = n_2 \approx (z_{1-\alpha/2} + z_\beta)^2 \left( \frac{2 s^2}{d^2} \right)$


<div class="red">
WTF? Seriously?
</div>
</div> 

## Sample size planning: key ideas
- My data is noisy ($s$), so the group with the higher average in the test is not always *actually* higher.   

- There are two mistakes you can make: 
    - Declare the treatments different, when they are the same (Type I)
    - Declare the treatment the same, when they are different (Type II)
    
- I want a low probability of both of those mistakes ($\alpha$, $\beta$) given a specific known difference between treatments ($d$)


## Sample size calculator

Sample size to detect at \$1 difference in average 30-day purchases. 
```{r}
sd(d$purch)
power.t.test(sd=sd(d$purch), delta=1, sig.level=0.95, power=0.80)
```
We need 3272 in each group. *Note:* Power is 1 minus probability we declare the treatment the same, when they are actually different.


## Sample size planning

There is a slightly different formula for 

**Continous outcomes (eg money, time-on-site)**  

$n_1 = n_2 \approx (z_{1-\alpha/2} + z_\beta)^2 \left( \frac{2 s^2}{d^2} \right)$

**Binary outcomes (eg conversions)**  

$n_1 = n_2 \approx (z_{1-\alpha/2} + z_\beta)^2 \left( \frac{2 p (1-p)}{d^2} \right)$


## Sample size calculator
**Binary outcomes**
```{r}
power.prop.test(p1=0.07, p2=0.07 + 0.01, sig.level=0.05, power=0.80)
```

## Sample size calculator

![](images/evans_sample_size.png){width=60%}


## A word of caution about sample size calculators

There are a lot of different ideas about sample size calculations floating around. These formulas differ on what assumptions they may about what you are trying to do, but it is very hard to figure out what assumptions are being made (even for experts). 

A decent sample size calculation will help you identify whether you are likely to end up with way too much or too little data. 


## Tips for getting started with A/B testing
- Keep it simple
- Be prepared to find no effect
- Choose "strong" treatments
- Run many tests in fast succession 
- You are searching for a few "golden tickets"

## Things you just learned (or reviewed)
- Three types of variables in test data
    - Treatment (x's)
    - Response (y's)
    - Baseline variables (z's)
- Analyzing tests with binary outcomes
    - Bar plot or mosaic plot
    - `prop.test()` for significance
- Analyzing tests with continuous outcomes
    - Dot plots or violin plots
    - `t.test()` for significance
- Eight key questions that define a test plan
- Sample size calculations
